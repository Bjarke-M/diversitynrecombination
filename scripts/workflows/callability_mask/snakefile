#workflow for assessing filtering the GENOME VCF files and generating BED files for each individual, only with the sites that pass the filters.
# Define a Python function to generate the expand statement
# Import the snakemake module for working with snakemake files
species_list = ['Daubentonia_madagascariensis','Saguinus_midas','Chlorocebus_aethiops','Rhinopithecus_roxellana',
                'Mandrillus_sphinx','Macaca_mulatta','Loris_tardigradus','Callithrix_jacchus',
                'Pithecia_pithecia','Lemur_catta-Thomas','Gorilla_gorilla_gorilla','Aotus_nancymaae','Sapajus_apella',
                'Pongo_abelii','Cercocebus_atys','Galago_moholi','Nomascus_leucogenys','Colobus_guereza',
                'Cebus_albifrons','Cercopithecus_mitis','Pongo_pygmaeus','Pan_troglodytes','Erythrocebus_patas',
                'Microcebus_murinus','Atele_fusciceps','Otolemur_garnettii','Nycticebus_pygmaeus']

#Missing g.vcf.gz species: Theropithecus_gelada 'Papio_anubis'
#Missing chainfiles: Carlito_syrichta Propithecus_coquereli'
# Define the list of species you want to process


input_file = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/PDGP_metadata.txt'
dictionary_of_inds = {}
with open(input_file, 'r') as file:
    header = next(file)  # Read and skip the header line
    for line in file:
        fields = line.strip().split(',')
        pdgp_id, genus, species, froh, sex, ref_assembly = fields
        if not ref_assembly:  # Check if ref_assembly is empty
            ref_assembly = 'unknown'
        # Add the pdgp_id to the appropriate category in the dictionary
        if ref_assembly not in dictionary_of_inds:
            dictionary_of_inds[ref_assembly] = [pdgp_id]
        else:
            dictionary_of_inds[ref_assembly].append(pdgp_id)


# Define a Python function to generate the expand statement
def generate_input_paths(species_list, lifted_bed):
    input_paths = []
    for species in species_list:
        if lifted_bed==0:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
        elif lifted_bed==1:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
        elif lifted_bed==2:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_lifted_sorted.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
    return input_paths


# Modify the rule all to use species_list for both original and lifted BED files
rule all:
    input:
        expand(generate_input_paths(species_list, lifted_bed=0), species=species_list),
        expand(generate_input_paths(species_list, lifted_bed=1), species=species_list),
        expand(generate_input_paths(species_list, lifted_bed=2), species=species_list)

# Rule to generate individual BED files based on specified criteria
rule generate_ind_beds:
    input:
        vcf="/home/bjarkemp/primatediversity/data/het_data_11_04_2022/{pdgp_id}/{pdgp_id}_concat.vcf.gz",
    output:
        output_file="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed",
        modcov_file="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_modcov.txt"
    params:
        MIN_HET_AD=3,  # Adjust this value as needed
        GQ=30  # Adjust this value as needed
    shell:
        """
        modcov=$(bcftools stats -d 2,500,1 {input.vcf} | grep 'DP' | \
        grep -iv -e '#' -e '<' -e '>' | sort -k 6 -V -r | head -1 | awk '{{print $3}}')
        echo $modcov > {output.modcov_file}
        min_cov=$((modcov / 2))
        max_cov=$((modcov * 2))
        # Define the output file path for the current individual
        # Apply filters and process variants, output to the defined file path
        bcftools view {input.vcf} | \
            bcftools filter -e "(GT='./.') | (GT='het' & FMT/AD[*:*] < {params.MIN_HET_AD} ) | FMT/DP <= $min_cov | FMT/DP >= $max_cov | FMT/GQ <= {params.GQ}" | \
            grep -v '#' | \
            awk 'BEGIN{{OFS="\\t"}}{{ print $1, $2-1, $2 }}' - | \
            bedtools merge | \
            sort -k1,1 -k2,2n | \
            bedtools merge > {output.output_file}
        """

#lift bed files to the human reference genome
rule liftover_generate_bed:
    input:
        chain_file="/home/bjarkemp/primatediversity/data/chain_files_15_03_2022/{species}_To_hg38.liftOver.gz",
        input_bed= '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed'
    output:
        bed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed",
        unmapbed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed.unmap"
    shell:
        """
        # Run CrossMap.py to perform liftover and generate VCF files
        CrossMap.py bed --chromid l {input.chain_file} {input.input_bed} {output.bed} --unmap-file {output.unmapbed}
        """



rule sort_lifted:
    input:
        input_bed= '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed'
    output:
        bed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_lifted_sorted.bed"
    shell:
        """
        sort -k1,1 -k2,2n {input.input_bed} > {output.bed}
        """


# rule intersect_callable:
#     input:
#         window_file = "/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/window_bed/{ref_assembly}/{species}/{species}_{window_size}.bed",        
#         callable_vcf = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{ref_assembly}/{pd_id}/{pd_id}_lifted_sorted.bed',
#         window_file_par = "/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/window_bed/{ref_assembly}/{species}/par/{species}_{window_size}_par.bed"
#     output:
#         outfile = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{ref_assembly}/{species}/{pd_id}/txt/{pd_id}_coverage_{window_size}.txt',
#         outfile_par = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{ref_assembly}/{species}/par/{pd_id}/txt/{pd_id}_coverage_{window_size}_par.txt'
#     conda:
#         'envs/bedtools.yaml'
#     log:
#         nonpar = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/scripts/workflows/mask/{ref_assembly}/{species}/{pd_id}/txt/log/{pd_id}_coverage_{window_size}.log',
#         par = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/scripts/workflows/mask/{ref_assembly}/{species}/par/{pd_id}/txt/log/{pd_id}_coverage_{window_size}_par.log'
#     shell:
#         '''
#         bedtools intersect -a {input.window_file} -b {input.callable_vcf} -wo > {output.outfile} 2> {log.nonpar}
#         bedtools intersect -a {input.window_file_par} -b {input.callable_vcf} -wo > {output.outfile_par} 2> {log.par}
#         '''

## NOTE: extract PAR regions from the callable vcf file, maybe i shoudl delete the par region from the bcf.gz file and then make a file only with the par regions and run the window pi seperately..
## maybe bcf view can be used in a way..?? look into this it might be easier

# rule calculate_coverage:
#     input:
#         ind_isolation_files = /home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/species_specific_bcfs/{ref_assembly}/{species}_isolation.txt
#     output:
#         masked_file = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{ref_assembly}/{species}/{pd_id}/csv/{pd_id}_masked_{window_size}.csv'
#     params:
#         window_size = "{window_size}",
#         pd_id = "{pd_id}",
#         sex = lambda wildcards: get_sex(wildcards.pd_id)
#     conda:
#         'envs/maskfile.yaml'
#     script: '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/scripts/workflows/calculate_win_based_coverage/maskfile.R'

# rule get_average_cov:
#     input:
#           expand
#         #isolation_file = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{ref_assembly}/{species}/{pd_id}/csv/{pd_id}_masked_{window_size}.csv'
#     output:
#         average_cov = "/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask_cov/{ref_assembly}/{species}_window_mask_cov.csv"
#     shell:
#         """
#         #get the average coverage for each species per window
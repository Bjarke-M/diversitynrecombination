#workflow for assessing filtering the GENOME VCF files and generating BED files for each individual, only with the sites that pass the filters.
# Define a Python function to generate the expand statement
# Import the snakemake module for working with snakemake files
#species_list = ['Daubentonia_madagascariensis','Saguinus_midas','Chlorocebus_aethiops',
                # 'Rhinopithecus_roxellana','Mandrillus_sphinx','Macaca_mulatta','Loris_tardigradus','Callithrix_jacchus',
                # 'Pithecia_pithecia','Lemur_catta-Thomas','Gorilla_gorilla_gorilla','Aotus_nancymaae','Sapajus_apella',
                # 'Pongo_abelii','Cercocebus_atys','Galago_moholi','Nomascus_leucogenys','Colobus_guereza',
                # 'Cebus_albifrons','Cercopithecus_mitis','Pongo_pygmaeus','Pan_troglodytes','Erythrocebus_patas',
                # 'Microcebus_murinus','Atele_fusciceps','Otolemur_garnettii','Nycticebus_pygmaeus']
species_list = ['Daubentonia_madagascariensis','Loris_tardigradus']

#Missing g.vcf.gz species: Theropithecus_gelada
#Missing chainfiles: Carlito_syrichta Propithecus_coquereli'
# Define the list of species you want to process


input_file = '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/PDGP_metadata.txt'
dictionary_of_inds = {}
with open(input_file, 'r') as file:
    header = next(file)  # Read and skip the header line
    for line in file:
        fields = line.strip().split(',')
        pdgp_id, genus, species, froh, sex, ref_assembly = fields
        if not ref_assembly:  # Check if ref_assembly is empty
            ref_assembly = 'unknown'
        # Add the pdgp_id to the appropriate category in the dictionary
        if ref_assembly not in dictionary_of_inds:
            dictionary_of_inds[ref_assembly] = [pdgp_id]
        else:
            dictionary_of_inds[ref_assembly].append(pdgp_id)


# Define a Python function to generate the expand statement
def generate_input_paths(species_list, lifted_bed):
    input_paths = []
    for species in species_list:
        if lifted_bed==0:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
        elif lifted_bed==1:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
        elif lifted_bed==2:
            input_paths.extend([
                f"/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_lifted_sorted.bed"
                for pdgp_id in dictionary_of_inds[species]
            ])
    return input_paths


# Modify the rule all to use species_list for both original and lifted BED files
rule all:
    input:
        expand(generate_input_paths(species_list, lifted_bed=0), species=species_list),
        expand(generate_input_paths(species_list, lifted_bed=1), species=species_list),
        expand(generate_input_paths(species_list, lifted_bed=2), species=species_list)

# Rule to generate individual BED files based on specified criteria
rule generate_ind_beds:
    input:
        vcf="/home/bjarkemp/primatediversity/data/het_data_11_04_2022/{pdgp_id}/{pdgp_id}_concat.vcf.gz",
    output:
        output_file="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed",
        modcov_file="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_modcov.txt"
    params:
        MIN_HET_AD=3,  # Adjust this value as needed
        GQ=30  # Adjust this value as needed
    shell:
        """
        modcov=$(bcftools stats -d 2,500,1 {input.vcf} | grep 'DP' | \
        grep -iv -e '#' -e '<' -e '>' | sort -k 6 -V -r | head -1 | awk '{{print $3}}')
        echo $modcov > {output.modcov_file}
        min_cov=$((modcov / 2))
        max_cov=$((modcov * 2))
        # Define the output file path for the current individual
        # Apply filters and process variants, output to the defined file path
        bcftools view {input.vcf} | \
            bcftools filter -e "(GT='./.') | (GT='het' & FMT/AD[*:*] < {params.MIN_HET_AD} ) | FMT/DP <= $min_cov | FMT/DP >= $max_cov | FMT/GQ <= {params.GQ}" | \
            grep -v '#' | \
            awk 'BEGIN{{OFS="\\t"}}{{ print $1, $2-1, $2 }}' - | \
            bedtools merge | \
            sort -k1,1 -k2,2n | \
            bedtools merge > {output.output_file}
        """

#lift bed files to the human reference genome
rule liftover_generate_bed:
    input:
        chain_file="/home/bjarkemp/primatediversity/data/chain_files_15_03_2022/{species}_To_hg38.liftOver.gz",
        input_bed= '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}.bed'
    output:
        bed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed",
        unmapbed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed.unmap"
    shell:
        """
        # Run CrossMap.py to perform liftover and generate VCF files
        CrossMap.py bed --chromid l {input.chain_file} {input.input_bed} {output.bed} --unmap-file {output.unmapbed}
        """



rule sort_lifted:
    input:
        input_bed= '/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_bed_lifted.bed'
    output:
        bed="/home/bjarkemp/primatediversity/people/bjarkemp/diversitynrecombination/data/mask/{species}/{pdgp_id}/{pdgp_id}_lifted_sorted.bed"
    shell:
        """
        sort -k1,1 -k2,2n {input.input_bed} > {output.bed}
        """